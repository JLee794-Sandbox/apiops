{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è OpenAI\n",
    "\n",
    "## Demo Overview\n",
    "\n",
    "This Jupyter Notebook demonstrates the integration of Azure API Management (APIM) with Azure OpenAI services. The demo covers various aspects including initial setup, JWT authentication, semantic caching, load balancing, and content safety testing. Each section is designed to showcase specific functionalities and performance metrics of the integrated services.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Initial Setup](#3)\n",
    "2. [JWT Authentication Testing](#jwt-authentication-testing)\n",
    "3. [Analyzing Semantic Caching](#sdk)\n",
    "    - [Set 1 - Exact Matches (Control Group)](#üß™-semantic-caching-set-1-exact-matches-control-group)\n",
    "    - [Set 2 - Paraphrased/Slightly Related Matches](#üß™-semantic-caching-set-2-paraphrasedslightly-related-matches)\n",
    "4. [Load Balancing Test](#load-balancing-test)\n",
    "    - [Weights and Priorities](#üß™-load-balancing-test-weights-and-priorities)\n",
    "5. [Summary Analysis](#summary-analysis)\n",
    "\n",
    "### Prerequisites\n",
    "- [Python 3.8 or later version](https://www.python.org/) installed\n",
    "- [Pandas Library](https://pandas.pydata.org/) and matplotlib installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) installed\n",
    "- [An Azure Subscription](https://azure.microsoft.com/en-us/free/) with Contributor permissions\n",
    "- [Access granted to Azure OpenAI](https://aka.ms/oai/access) or just enable the mock service\n",
    "- [Sign in to Azure with Azure CLI](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli-interactively)\n",
    "- Azure API Management Resource you have access to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 1Ô∏è‚É£ Initial Setup\n",
    "\n",
    "In this section, we set up the environment variables and test the basic network connections to ensure everything is configured correctly. This includes loading environment variables, setting up API endpoints, and verifying the connection to the Azure OpenAI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path='./.env')\n",
    "\n",
    "# Set the local env vars\n",
    "apim_base_url = os.getenv('APIM_BASE_URL')\n",
    "apim_api = os.getenv('APIM_API')\n",
    "apim_subscription_key = os.getenv('APIM_SUBSCRIPTION_KEY').strip()\n",
    "openai_api_version = os.getenv('OPENAI_API_VERSION')\n",
    "openai_model_name = os.getenv('OPENAI_MODEL_NAME')\n",
    "openai_deployment_name = os.getenv('OPENAI_DEPLOYMENT_NAME')\n",
    "frontend_client_id = os.getenv('CLIENT_ID')\n",
    "tenant_id = os.getenv('TENANT_ID')\n",
    "\n",
    "# Set the OpenAI API URL (https://<apim_base_url>/<api>)\n",
    "apim_resource_gateway_url = f\"{apim_base_url}/{apim_api}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Test the connection to ensure the environment variables are set correctly\n",
    "try:\n",
    "    print(\"Azure APIM Base URL: \", apim_base_url)\n",
    "    print(\"Azure OpenAI Gateway URL: \", apim_resource_gateway_url)\n",
    "    print(\"Azure OpenAI Gateway Path: \", apim_api)\n",
    "    print(\"Azure OpenAI Deployment Name: \", openai_deployment_name)\n",
    "    print(\"Azure OpenAI API Version: \", openai_api_version)\n",
    "    \n",
    "    # Check if all required environment variables are set\n",
    "    if not all([apim_base_url, apim_api, apim_subscription_key, openai_api_version, openai_model_name, openai_deployment_name]):\n",
    "        raise ValueError(\"One or more environment variables are missing. Please check your .env file.\")\n",
    "    \n",
    "    client = AzureOpenAI(azure_endpoint=apim_resource_gateway_url, api_key=apim_subscription_key, api_version=openai_api_version)\n",
    "    response = client.chat.completions.create(model=openai_model_name, messages=[{\"role\": \"system\", \"content\": \"Test connection\"}])\n",
    "    print(\"Connection successful. Response received.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "    if \"401\" in str(e):\n",
    "        print(\"‚ùå‚ùå‚ùå APIM requires a valid JWT access token. Please ensure your token is correct and try again. ‚ùå‚ùå‚ùå\")\n",
    "    elif \"404\" in str(e):\n",
    "        print(\"‚ùå‚ùå‚ùå The requested resource was not found. Please check the URL and endpoint configuration. ‚ùå‚ùå‚ùå\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîê JWT Authentication Testing\n",
    "\n",
    "In this section, we will test the JWT (JSON Web Token) authentication mechanism integrated with Azure API Management (APIM). JWT is a compact, URL-safe means of representing claims to be transferred between two parties. It is commonly used for authentication and authorization purposes.\n",
    "\n",
    "#### Overview\n",
    "\n",
    "The JWT authentication testing will cover the following aspects:\n",
    "\n",
    "1. __Token Generation__: How to generate a JWT token with the necessary claims.\n",
    "1. __Token Validation__: How APIM validates the JWT token to ensure it is authentic and has not been tampered with.\n",
    "1. __Access Control__: How to use JWT tokens to control access to different API endpoints based on the claims within the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the access token using the device flow\n",
    "# Requires setting the CLIENT_ID and TENANT_ID environment variables in your .env file\n",
    "\n",
    "import msal\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "url = apim_resource_gateway_url + \"/openai/deployments/\" + openai_deployment_name + \"/chat/completions?api-version=\" + openai_api_version\n",
    "\n",
    "\n",
    "# Create a public client application\n",
    "app = msal.PublicClientApplication(frontend_client_id, authority=f\"https://login.microsoftonline.com/{tenant_id}\")\n",
    "\n",
    "# Initiate the device flow\n",
    "flow = app.initiate_device_flow(scopes=[\"User.Read\"])\n",
    "if \"user_code\" not in flow:\n",
    "    raise ValueError(\"Failed to create device flow. Error: %s\" % json.dumps(flow, indent=2))\n",
    "\n",
    "print(flow[\"message\"])\n",
    "\n",
    "# Acquire token by device flow\n",
    "result = app.acquire_token_by_device_flow(flow)\n",
    "access_token = None\n",
    "if \"access_token\" in result:\n",
    "    access_token = result['access_token']\n",
    "    # Calling graph using the access token\n",
    "    graph_data = requests.get(\n",
    "        \"https://graph.microsoft.com/v1.0/me\",\n",
    "        headers={'Authorization': 'Bearer ' + access_token},\n",
    "    ).json()\n",
    "    print(\"Graph API call result: %s\" % json.dumps(graph_data, indent=2))\n",
    "else:\n",
    "    print(result.get(\"error\"))\n",
    "    print(result.get(\"error_description\"))\n",
    "    print(result.get(\"correlation_id\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from json import loads\n",
    "\n",
    "messages={\"messages\":[\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "]}\n",
    "\n",
    "response = requests.post(url, headers = {'api-key':apim_subscription_key, 'Authorization': 'Bearer ' + access_token}, json = messages)\n",
    "print(\"status code: \", response.status_code)\n",
    "if (response.status_code == 200):\n",
    "    data = json.loads(response.text)\n",
    "    print(\"response: \", data.get(\"choices\")[0].get(\"message\").get(\"content\"))\n",
    "    print(\"üéâüéâüéâ Successfully authenticated with JWT using Entra ID! üéâüéâüéâ\")\n",
    "else:\n",
    "    print(response.text)\n",
    "    print(\"‚ùå‚ùå‚ùå Failed to authenticate with JWT using Entra ID! ‚ùå‚ùå‚ùå\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Analyzing Semantic Caching\n",
    "\n",
    "This section consists of 4 sets of questions to demonstrate how semantic caching works within APIM.\n",
    "\n",
    "This Jupyter Notebook demonstrates how to integrate Azure Redis Cache with Azure API Management to cache responses from an embeddings model deployment. The steps are based on the official Microsoft documentation for caching external responses in Azure API Management.\n",
    "\n",
    "#### Instructions\n",
    "1. Connect an Azure Redis Cache instance to your API Management instance:\n",
    "    - Navigate to the Azure Portal.\n",
    "    - Go to Deployment & Infrastructure > External Cache.\n",
    "    - Follow the prompts to connect your Redis Cache instance.\n",
    "\n",
    "2. Create a backend for your embeddings model deployment:\n",
    "    - This involves setting up the backend service that will handle requests for your embeddings model.\n",
    "\n",
    "3. Add the following policy snippet to your inbound policy in Azure API Management:\n",
    "    ```xml\n",
    "    <azure-openai-semantic-cache-lookup score-threshold=\"0.8\" embeddings-backend-id=\"<embeddings-backend-name>\" embeddings-backend-auth=\"system-assigned\" />\n",
    "    ```\n",
    "    - `score-threshold`: A value between 0.0 and 1.0 that determines the similarity tolerance for cache matches. A higher score requires higher similarity.\n",
    "    - `embeddings-backend-id`: The identifier for your embeddings backend service.\n",
    "    - `embeddings-backend-auth`: Authentication method for the backend service, typically \"system-assigned\".\n",
    "\n",
    "4. Run through each test provided in the notebook and analyze the results to ensure the caching mechanism works as expected.\n",
    "\n",
    "For more detailed information, refer to the official documentation: [Azure API Management - How to Cache External Responses](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-cache-external)\n",
    "requiring higher similarity).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# Configure logging\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Executes a series of queries to an Azure OpenAI service and measures the response times.\n",
    "\n",
    "Parameters:\n",
    "questions (list): A list of questions to randomly select from for each query.\n",
    "runs (int): The number of times to run the queries. Default is 5.\n",
    "\n",
    "Returns:\n",
    "list: A list of response times for each query run.\n",
    "\n",
    "The function performs the following steps:\n",
    "1. Initializes an AzureOpenAI client with the provided endpoint, API key, and API version.\n",
    "2. For each run, selects a random question from the provided list.\n",
    "3. Constructs a message with a predefined system role and the selected question as the user content.\n",
    "4. Measures the time taken to get a response from the Azure OpenAI service.\n",
    "5. Handles exceptions, specifically checking for content filtering errors.\n",
    "6. Prints the response time, the question, and the response content.\n",
    "7. Appends the response time to a list and returns this list after all runs are completed.\n",
    "\"\"\"\n",
    "def run_queries(questions, runs=5, randomize=True):\n",
    "\n",
    "    api_runs = []\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=apim_resource_gateway_url, \n",
    "        api_key=apim_subscription_key, \n",
    "        api_version=openai_api_version\n",
    "    )\n",
    "\n",
    "    for i in range(runs):\n",
    "        if randomize:\n",
    "            random_question = random.choice(questions)\n",
    "        else:\n",
    "            random_question = questions[i % len(questions)]\n",
    "        random_question = random.choice(questions)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": random_question}\n",
    "        ]\n",
    "        region = \"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            raw_response = client.chat.completions.with_raw_response.create(\n",
    "                model=openai_model_name, \n",
    "                messages=messages, \n",
    "                extra_headers={\"Authorization\": \"Bearer \" + access_token}\n",
    "            )\n",
    "            response = raw_response.parse()\n",
    "            headers = raw_response.headers\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            remaining_tokens = headers.get('x-ratelimit-remaining-tokens')\n",
    "            remaining_requests = headers.get('x-ratelimit-remaining-requests')\n",
    "            region = headers.get('x-ms-region')\n",
    "            \n",
    "            logging.debug(f\"Response: {response}\")\n",
    "            logging.debug(f\"\\n\\tRemaining Tokens: {remaining_tokens} \\n\\tRemaining Requests: {remaining_requests}\\n\\tRegion: {region}\")\n",
    "            logging.debug(f\"\\tResponse Time: {response_time:.2f} seconds\")\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            if 'Azure AI Content Safety service' in str(e):\n",
    "                print(f\"\\n‚ùå Content Filtered: {e}\\n\\tInitial Question: {random_question}\\n\")\n",
    "            else:\n",
    "                print(f\"Error during API call: {e}\")\n",
    "            continue\n",
    "\n",
    "        response_time = time.time() - start_time\n",
    "        print(\"\\n\\n‚ñ∂Ô∏è Run: \", i+1, f\"duration: {response_time:.2f} seconds\")\n",
    "        if remaining_tokens:\n",
    "            print(\"\\tx-ratelimit-remaining-tokens:\", '\\x1b[1;31m'+remaining_tokens+'\\x1b[0m')\n",
    "        if remaining_requests:\n",
    "            print(\"\\tx-ratelimit-remaining-requests:\", '\\x1b[1;31m'+remaining_requests+'\\x1b[0m')\n",
    "        if region:\n",
    "            print(\"\\tx-ms-region:\", '\\x1b[1;31m'+region+'\\x1b[0m') # this header is useful to determine the region of the backend that served the request\n",
    "        if  headers.get('remaining-tokens'):\n",
    "            print(\"\\tremaining-tokens:\", '\\x1b[1;31m'+headers.get('remaining-tokens')+'\\x1b[0m')\n",
    "        if headers.get('consumed-tokens'):\n",
    "            print(\"\\tconsumed-tokens:\", '\\x1b[1;31m'+headers.get('consumed-tokens')+'\\x1b[0m')\n",
    "        print(\"üí¨ \", random_question, response.choices[0].message.content)\n",
    "\n",
    "        api_runs.append((response_time, region))\n",
    "\n",
    "    return api_runs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Reusable question function for testing\n",
    "Plots the response times of API runs using a bar chart.\n",
    "\n",
    "api_runs (list): A list of response times for each query run.\n",
    "\n",
    "1. Sets the figure size for the plot.\n",
    "2. Converts the list of response times into a pandas DataFrame.\n",
    "3. Adds a 'Run' column to the DataFrame to represent each run number.\n",
    "4. Plots a bar chart with 'Run' on the x-axis and 'Response Time' on the y-axis.\n",
    "5. Sets the title, x-axis label, and y-axis label for the plot.\n",
    "6. Sets the x-axis ticks to be the run numbers.\n",
    "Plots the response times of API runs using a bar chart.\n",
    "\n",
    "api_runs (list): A list of response times for each query run.\n",
    "\n",
    "1. Sets the figure size for the plot.\n",
    "2. Converts the list of response times into a pandas DataFrame.\n",
    "3. Adds a 'Run' column to the DataFrame to represent\n",
    "\"\"\"\n",
    "def plot_results(api_runs, title='Semantic Caching Performance'):\n",
    "    mpl.rcParams['figure.figsize'] = [15, 5]\n",
    "    df = pd.DataFrame(api_runs, columns=['Response Time'])\n",
    "    df['Run'] = range(1, len(df) + 1)\n",
    "    df.plot(kind='bar', x='Run', y='Response Time', legend=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Runs')\n",
    "    plt.ylabel('Response Time (s)')\n",
    "    plt.xticks(df['Run'], rotation=0)  # Set x-axis ticks to be the run numbers\n",
    "\n",
    "    average = df['Response Time'].mean()\n",
    "    plt.axhline(y=average, color='r', linestyle='--', label=f'Average: {average:.2f}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ Semantic Caching: Set 1 - Exact Matches (Control Group)\n",
    "\n",
    "This subsection of semantic caching is responsible for testing the first set of questions (Set 1: Exact Matches).\n",
    "\n",
    "The purpose of this test is to evaluate the performance and accuracy of the caching mechanism when dealing with exact match queries. This set serves as the control group, providing a baseline for comparison with other sets of questions that may involve more complex query patterns.\n",
    "\n",
    "__Expected Outcome__:\n",
    "- Latency should be extremely low for repeated queries.\n",
    "\n",
    "The results from this test will help in understanding the efficiency of the semantic caching system in handling straightforward, exact match queries and will serve as a reference point for further optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1\n",
    "api_runs = []  # Response Times for each run\n",
    "\n",
    "# Set 1: Exact Matches (Control Group)\n",
    "    # Expected Outcome:\n",
    "\t# ‚Ä¢\tLatency should be extremely low for repeated queries.\n",
    "questions = [\n",
    "    \"What is climate change?\",\n",
    "    \"What is climate change?\",\n",
    "    \"What is climate change?\",\n",
    "    \"What is climate change?\",\n",
    "    \"What is climate change?\"\n",
    "]\n",
    "\n",
    "api_runs = run_queries(questions)\n",
    "response_times = [run[0] for run in api_runs]\n",
    "plot_results(response_times, 'Semantic Caching Performance (Set 1: Exact Matches)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ Semantic Caching: Set 2: Paraphrased/Slightly Related Matches\n",
    "\n",
    "This subsection of semantic caching is responsible for testing the first set of questions (Set 2: Paraphrased Matches)\n",
    "\n",
    "In this test, we evaluate the performance and accuracy of the caching mechanism when dealing with paraphrased queries. This set helps in understanding how well the caching system handles variations in query phrasing while still retrieving relevant cached responses.\n",
    "\n",
    "__Expected Outcome__:\n",
    "- Similar questions (1, 2, 3) should hit the cache.\n",
    "- Slightly reworded but related questions (4) may still hit the cache if the similarity threshold allows.\n",
    "- Questions with overlapping meanings (5 and 6) might hit the cache if configured for broad similarity.\n",
    "- Question 7 might generate a new response depending on threshold settings.\n",
    "\n",
    "The results from this test will provide insights into the robustness of the semantic caching system in handling paraphrased queries and its ability to maintain low latency and high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_runs = []  # Response Times for each run\n",
    "runs = 6\n",
    "questions = [\n",
    "    \"What is climate change?\",\n",
    "    \"Explain climate change.\",\n",
    "    \"Describe the concept of climate change.\",\n",
    "    \"Tell me about global warming and its impact on the environment.\",\n",
    "    \"What causes climate change?\",\n",
    "    \"How does carbon dioxide contribute to global warming?\",\n",
    "    \"What are the effects of climate change on sea levels?\"\n",
    "]\n",
    "\n",
    "api_runs = run_queries(questions, runs, randomize=False) \n",
    "response_times = [run[0] for run in api_runs]\n",
    "plot_results(response_times, 'Semantic Caching Performance (Set 2: Paraphrased Matches)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Load Balancing Test\n",
    "\n",
    "In this section, we test the load balancing capabilities of the Azure OpenAI service. The test involves sending multiple requests to the service and monitoring the distribution of these requests across different backend servers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_load_balancing_results(api_runs, color_map, title='Load Balancing results'):\n",
    "    if color_map is None:\n",
    "        color_map = {\n",
    "            'East US': 'blue',\n",
    "            'North Central US': 'green',\n",
    "            'West US': 'red',\n",
    "            # Add more regions and their corresponding colors as needed\n",
    "        }\n",
    "    mpl.rcParams['figure.figsize'] = [15, 7]\n",
    "    df = pd.DataFrame(api_runs, columns=['Response Time', 'Region'])\n",
    "    df['Run'] = range(1, len(df) + 1)\n",
    "\n",
    "    # Plot the dataframe with colored bars\n",
    "    ax = df.plot(kind='bar', x='Run', y='Response Time', color=[color_map.get(region, 'gray') for region in df['Region']], legend=False)\n",
    "\n",
    "    # Add legend\n",
    "    legend_labels = [plt.Rectangle((0, 0), 1, 1, color=color_map.get(region, 'gray')) for region in df['Region'].unique()]\n",
    "    ax.legend(legend_labels, df['Region'].unique())\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Runs')\n",
    "    plt.ylabel('Response Time')\n",
    "    plt.xticks(df['Run'], rotation=0)\n",
    "\n",
    "    average = df['Response Time'].mean()\n",
    "    plt.axhline(y=average, color='r', linestyle='--', label=f'Average: {average:.2f}')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Load Balancing Test: Weights and Priorities\n",
    "\n",
    "In this section, we test the load balancing capabilities of the Azure OpenAI service with a focus on weights and priorities. The test involves sending multiple requests to the service and monitoring the distribution of these requests across different backend servers based on their assigned weights and priorities.\n",
    "\n",
    "#### Expected Outcome:\n",
    "- Requests should be distributed according to the weights and priorities assigned to each backend server.\n",
    "- Higher priority servers should handle more requests.\n",
    "- Response times should be consistent, indicating effective load balancing.\n",
    "\n",
    "#### Steps:\n",
    "1. **Send Requests**: Send multiple requests to the Azure OpenAI API and record the backend server handling each request.\n",
    "2. **Monitor Distribution**: Analyze the distribution of requests across the backend servers to ensure they align with the assigned weights and priorities.\n",
    "3. **Plot Results**: Visualize the response times and distribution of requests across different backend servers.\n",
    "\n",
    "#### Monitoring Load Balancing:\n",
    "- **Server Distribution**: The `plot_load_balancing_results` function will show the number of requests handled by each backend server.\n",
    "- **Response Times**: Consistent response times across requests indicate effective load balancing.\n",
    "- **Logs and Metrics**: Use Azure Monitor and Application Insights to track logs and metrics related to request distribution and performance.\n",
    "\n",
    "By analyzing the server distribution and response times, you can ensure that the load balancing mechanism is working effectively and that the requests are distributed according to the assigned weights and priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_runs = []  \n",
    "runs = 20\n",
    "questions = [\n",
    "    \"What is climate change?\",\n",
    "    \"Explain climate change.\",\n",
    "    \"Describe the concept of climate change.\",\n",
    "    \"Tell me about global warming and its impact on the environment.\",\n",
    "    \"What causes climate change?\",\n",
    "    \"How does carbon dioxide contribute to global warming?\",\n",
    "    \"What are the effects of climate change on sea levels?\"\n",
    "]\n",
    "\n",
    "api_runs = run_queries(questions, runs, randomize=True) \n",
    "# Define the color map\n",
    "color_map = {\n",
    "    'East US': 'blue',\n",
    "    'North Central US': 'green',\n",
    "    'West US': 'red'\n",
    "    # Add more regions and their corresponding colors as needed\n",
    "}\n",
    "\n",
    "plot_load_balancing_results(api_runs, color_map, 'Load Balancing results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Extract response times and regions from api_runs\n",
    "response_times = [run[0] for run in api_runs]\n",
    "regions = [run[1] for run in api_runs]\n",
    "\n",
    "plot_results(response_times, 'Semantic Caching Performance (Set 2: Paraphrased Matches)')\n",
    "# Calculate % distribution of load to the backend regions\n",
    "region_counts = pd.Series(regions).value_counts(normalize=True) * 100\n",
    "print(\"\\nüìä Percentage distribution of load to backend regions:\")\n",
    "print(region_counts)\n",
    "\n",
    "# Calculate average response times per region\n",
    "region_avg_response_times = pd.DataFrame(api_runs, columns=['Response Time', 'Region']).groupby('Region').mean()\n",
    "print(\"\\n‚è±Ô∏è Average response times per region:\")\n",
    "print(region_avg_response_times)\n",
    "\n",
    "# Identify potential causes for long response times\n",
    "threshold = np.percentile(response_times, 90)  # Define a threshold for long response times (90th percentile)\n",
    "long_response_times = [(time, region) for time, region in api_runs if time > threshold]\n",
    "\n",
    "print(\"\\nüö® Potential causes for long response times:\")\n",
    "for time, region in long_response_times:\n",
    "    print(f\"‚è∞ Response Time: {time:.2f} seconds, üåç Region: {region}\")\n",
    "    if time > 10:  # Arbitrary threshold for rate limiting/throttling\n",
    "        print(\"  - ‚ö†Ô∏è Potential cause: Rate limiting/throttling or policy misconfiguration\")\n",
    "\n",
    "        # Analyze time gaps between region calls to identify potential circuit breaker events\n",
    "        time_gaps = [j - i for i, j in zip(response_times[:-1], response_times[1:])]\n",
    "        circuit_breaker_events = [(gap, regions[idx + 1]) for idx, gap in enumerate(time_gaps) if gap > 5]\n",
    "        if circuit_breaker_events:\n",
    "            print(\"\\nüîç Circuit Breaker Analysis based on time gaps:\")\n",
    "            for gap, region in circuit_breaker_events:\n",
    "                print(f\"‚è∞ Time Gap: {gap:.2f} seconds, üåç Region: {region}\")\n",
    "                print(\"  - ‚ö†Ô∏è Potential cause: Circuit breaker triggered due to high response time gap\")\n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-gateway",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
