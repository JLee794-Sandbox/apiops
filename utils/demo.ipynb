{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è OpenAI\n",
    "\n",
    "## Demo Overview\n",
    "\n",
    "This Jupyter Notebook demonstrates the integration of Azure API Management (APIM) with Azure OpenAI services. The demo covers various aspects including initial setup, JWT authentication, semantic caching, load balancing, and content safety testing. Each section is designed to showcase specific functionalities and performance metrics of the integrated services.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Initial Setup](#3)\n",
    "2. [JWT Authentication Testing](#jwt-authentication-testing)\n",
    "3. [Analyzing Semantic Caching](#sdk)\n",
    "    - [Set 1 - Exact Matches (Control Group)](#üß™-semantic-caching-set-1-exact-matches-control-group)\n",
    "    - [Set 2 - Paraphrased/Slightly Related Matches](#üß™-semantic-caching-set-2-paraphrasedslightly-related-matches)\n",
    "4. [Load Balancing Test](#load-balancing-test)\n",
    "    - [Weights and Priorities](#üß™-load-balancing-test-weights-and-priorities)\n",
    "5. [Summary Analysis](#summary-analysis)\n",
    "\n",
    "### Prerequisites\n",
    "- [Python 3.8 or later version](https://www.python.org/) installed\n",
    "- [Pandas Library](https://pandas.pydata.org/) and matplotlib installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) installed\n",
    "- [An Azure Subscription](https://azure.microsoft.com/en-us/free/) with Contributor permissions\n",
    "- [Access granted to Azure OpenAI](https://aka.ms/oai/access) or just enable the mock service\n",
    "- [Sign in to Azure with Azure CLI](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli-interactively)\n",
    "- Azure API Management Resource you have access to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 1Ô∏è‚É£ Initial Setup\n",
    "\n",
    "In this section, we set up the environment variables and test the basic network connections to ensure everything is configured correctly. This includes loading environment variables, setting up API endpoints, and verifying the connection to the Azure OpenAI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logging\n",
    "from helper import initialize_logging\n",
    "import logging\n",
    "\n",
    "LOG_LEVEL = logging.DEBUG\n",
    "logger = initialize_logging(LOG_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Environment Variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path='./.env')\n",
    "\n",
    "# Set the local env vars\n",
    "apim_base_url = os.getenv('APIM_BASE_URL')\n",
    "apim_api = os.getenv('APIM_API')\n",
    "apim_subscription_key = os.getenv('APIM_SUBSCRIPTION_KEY').strip()\n",
    "openai_api_version = os.getenv('OPENAI_API_VERSION')\n",
    "openai_model_name = os.getenv('OPENAI_MODEL_NAME')\n",
    "openai_deployment_name = os.getenv('OPENAI_DEPLOYMENT_NAME')\n",
    "frontend_client_id = os.getenv('CLIENT_ID')\n",
    "tenant_id = os.getenv('TENANT_ID')\n",
    "\n",
    "# Set the OpenAI API URL (https://<apim_base_url>/<api>)\n",
    "apim_resource_gateway_url = f\"{apim_base_url}/{apim_api}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the endpoint with just the APIM subscription key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mDEBUG   \u001b[0m \u001b[34mResponse: ChatCompletion(id='chatcmpl-Ab58yR1arGd098ys26mxXmWB6je1d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Connection successful! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1733401272, model='gpt-4o-mini', object='chat.completion', service_tier=None, system_fingerprint='fp_04751d0b65', usage=CompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19, completion_tokens_details=None, prompt_tokens_details=None), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful. Response received.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Test the connection to ensure the environment variables are set correctly\n",
    "# If you see a constant 404 or other errors:\n",
    "#   - check to make sure the API URL suffix includes the `/openai` path\n",
    "#   - check the backend config to have ssl validation enabled\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=apim_resource_gateway_url, \n",
    "        api_key=apim_subscription_key, \n",
    "        api_version=openai_api_version\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=openai_model_name, \n",
    "        messages=[{\"role\": \"system\", \"content\": \"Test connection\"}]\n",
    "    )\n",
    "    print(\"Connection successful. Response received.\")\n",
    "    logger.debug(f\"Response: {response}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "    logger.error(f\"Connection failed: {e}\")\n",
    "    if \"401\" in str(e):\n",
    "        print(\"‚ùå‚ùå‚ùå APIM requires a valid JWT access token and a valid Subscription Key. Please ensure your token/key is correct and try again. ‚ùå‚ùå‚ùå\")\n",
    "    elif \"404\" in str(e):\n",
    "        print(\"‚ùå‚ùå‚ùå The requested resource was not found. Please check the URL and endpoint configuration. ‚ùå‚ùå‚ùå\")\n",
    "    elif \"timeout\" in str(e).lower():\n",
    "        print(\"‚ùå‚ùå‚ùå The connection timed out. Please check your network connection and try again. ‚ùå‚ùå‚ùå\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîê JWT Authentication Testing\n",
    "\n",
    "In this section, we will test the JWT (JSON Web Token) authentication mechanism integrated with Azure API Management (APIM). JWT is a compact, URL-safe means of representing claims to be transferred between two parties. It is commonly used for authentication and authorization purposes.\n",
    "\n",
    "[![flow](../images/access-controlling.gif)](access-controlling.ipynb)\n",
    "\n",
    "\n",
    "#### Overview\n",
    "\n",
    "The JWT authentication testing will cover the following aspects:\n",
    "\n",
    "1. __Token Generation__: How to generate a JWT token with the necessary claims.\n",
    "1. __Token Validation__: How APIM validates the JWT token to ensure it is authentic and has not been tampered with.\n",
    "1. __Access Control__: How to use JWT tokens to control access to different API endpoints based on the claims within the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the access token using the device flow\n",
    "# Requires setting the CLIENT_ID and TENANT_ID environment variables in your .env file\n",
    "\n",
    "import msal\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = apim_resource_gateway_url + \"/openai/deployments/\" + openai_deployment_name + \"/chat/completions?api-version=\" + openai_api_version\n",
    "\n",
    "app = msal.PublicClientApplication(frontend_client_id, authority=f\"https://login.microsoftonline.com/{tenant_id}\")\n",
    "\n",
    "# Initiate the device flow\n",
    "flow = app.initiate_device_flow(scopes=[\"User.Read\"])\n",
    "if \"user_code\" not in flow:\n",
    "    raise ValueError(\"Failed to create device flow. Error: %s\" % json.dumps(flow, indent=2))\n",
    "\n",
    "print(flow[\"message\"])\n",
    "\n",
    "# Acquire token by device flow\n",
    "result = app.acquire_token_by_device_flow(flow)\n",
    "access_token = None\n",
    "if \"access_token\" in result:\n",
    "    access_token = result['access_token']\n",
    "    # Calling graph using the access token\n",
    "    graph_data = requests.get(\n",
    "        \"https://graph.microsoft.com/v1.0/me\",\n",
    "        headers={'Authorization': 'Bearer ' + access_token},\n",
    "    ).json()\n",
    "    print(\"Graph API call result: %s\" % json.dumps(graph_data, indent=2))\n",
    "else:\n",
    "    logger.debug(result.get(\"error\"))\n",
    "    logger.debug(result.get(\"error_description\"))\n",
    "    logger.debug(result.get(\"correlation_id\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "messages={\"messages\":[\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "]}\n",
    "\n",
    "response = requests.post(url, headers = {'api-key':apim_subscription_key, 'Authorization': 'Bearer ' + access_token}, json = messages)\n",
    "\n",
    "logging.info(\"status code: \", response.status_code)\n",
    "\n",
    "if (response.status_code == 200):\n",
    "    logger.info(response.headers.get(\"x-ms-region\"))\n",
    "    remaining_tokens = response.headers.get(\"x-ratelimit-remaining-tokens\")\n",
    "    remaining_requests = response.headers.get(\"x-ratelimit-remaining-requests\")\n",
    "    \n",
    "    if remaining_tokens:\n",
    "        logger.info(f\"Remaining Tokens: {remaining_tokens}\")\n",
    "    if remaining_requests:\n",
    "        logger.info(f\"Remaining Requests: {remaining_requests}\")\n",
    "    \n",
    "    data = json.loads(response.text)\n",
    "    print(\"response: \", data.get(\"choices\")[0].get(\"message\").get(\"content\"))\n",
    "    print(\"üéâüéâüéâ Successfully authenticated with JWT using Entra ID! üéâüéâüéâ\")\n",
    "else:\n",
    "    logger.error(response.text)\n",
    "    print(\"‚ùå‚ùå‚ùå Failed to authenticate with JWT using Entra ID! ‚ùå‚ùå‚ùå\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize APIClient\n",
    "\n",
    "We will use the helper methods to send requests from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import APIClient\n",
    "\n",
    "client = APIClient(log_level=LOG_LEVEL if 'LOG_LEVEL' in globals() else logging.INFO) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backend Circuit Breaker\n",
    "![flow](../images/backend-circuit-breaking.gif)\n",
    "\n",
    "Playground to try the built-in [backend circuit breaker functionality of APIM](https://learn.microsoft.com/en-us/azure/api-management/backends?tabs=bicep) \n",
    "\n",
    "For the sake of the demo, we will be leveraging the apiOps toolkit to add a circuit breaker policy and test its results.\n",
    "1. Append to the `backendInformation.json` of your target backend the `circuitBreaker` property like so:\n",
    "    ```json\n",
    "    \"circuitBreaker\": {\n",
    "        \"rules\": [\n",
    "            {\n",
    "                \"failureCondition\": {\n",
    "                    \"count\": 3,\n",
    "                    \"errorReasons\": [\n",
    "                        \"Server errors\"\n",
    "                    ],\n",
    "                    \"interval\": \"PT5M\",\n",
    "                    \"statusCodeRanges\": [\n",
    "                        {\n",
    "                        \"min\": 429,\n",
    "                        \"max\": 429\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"name\": \"myBreakerRule\",\n",
    "                \"tripDuration\": \"PT1M\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    ```\n",
    "    > Where you can define multiple circuit breaker rules and conditions such as the number of failure conditions (`count`) within a defined time `interval` and a range of status codes (`statusCodeRanges`) indicating failures.\n",
    "\n",
    "2. Once published to APIM, ensure your API policy is routing to the backend with the circuit breaker\n",
    "3. Set the token limit from the Azure OpenAI model to a small value such as 1K for testing\n",
    "\n",
    "If the circuitbreaker is working as intended, you should see 60s delay between requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helper import run_queries, plot_results\n",
    "runs = 10\n",
    "api_runs = []  # Response Times for each run\n",
    "\n",
    "# Set 1: Exact Matches (Control Group)\n",
    "    # Expected Outcome:\n",
    "\t# ‚Ä¢\tLatency should be extremely low for repeated queries.\n",
    "questions = [\n",
    "    \"What is climate change?\",\n",
    "    \"What is climate change?\",\n",
    "    \"What is climate change?\",\n",
    "    \"What is climate change?\",\n",
    "    \"What is climate change?\"\n",
    "]\n",
    "\n",
    "api_runs = client.run_queries(questions, runs)\n",
    "response_times = [run[0] for run in api_runs]\n",
    "client.plot_results(response_times, 'Circuit Breaker Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Load Balancing Test: Weights and Priorities\n",
    "\n",
    "![flow](../images/backend-pool-load-balancing.gif)\n",
    "\n",
    "In this section, we test the load balancing capabilities of the Azure OpenAI service with a focus on weights and priorities. The test involves sending multiple requests to the service and monitoring the distribution of these requests across different backend servers based on their assigned weights and priorities.\n",
    "\n",
    "#### Expected Outcome:\n",
    "- Requests should be distributed according to the weights and priorities assigned to each backend server.\n",
    "- Higher priority servers should handle more requests.\n",
    "- Response times should be consistent, indicating effective load balancing.\n",
    "\n",
    "#### Steps:\n",
    "1. **Send Requests**: Send multiple requests to the Azure OpenAI API and record the backend server handling each request.\n",
    "2. **Monitor Distribution**: Analyze the distribution of requests across the backend servers to ensure they align with the assigned weights and priorities.\n",
    "3. **Plot Results**: Visualize the response times and distribution of requests across different backend servers.\n",
    "\n",
    "#### Monitoring Load Balancing:\n",
    "- **Server Distribution**: The `plot_load_balancing_results` function will show the number of requests handled by each backend server.\n",
    "- **Response Times**: Consistent response times across requests indicate effective load balancing.\n",
    "- **Logs and Metrics**: Use Azure Monitor and Application Insights to track logs and metrics related to request distribution and performance.\n",
    "\n",
    "By analyzing the server distribution and response times, you can ensure that the load balancing mechanism is working effectively and that the requests are distributed according to the assigned weights and priorities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Balancing Options:\n",
    "- **Round-robin**: By default, requests are distributed evenly across the backends in the pool.\n",
    "- **Weighted**: Weights are assigned to the backends in the pool, and requests are distributed across the backends based on the relative weight assigned to each backend. Use this option for scenarios such as conducting a blue-green deployment.\n",
    "- **Priority-based**: Backends are organized in priority groups, and requests are sent to the backends in order of the priority groups. Within a priority group, requests are distributed either evenly across the backends, or (if assigned) according to the relative weight assigned to each backend.\n",
    "\n",
    "For this demo, we will be referencing the backend load balancing pool config outlined in the [openai-backend-pool](../apimartifacts/backends/openai-backend-pool/backendInformation.json)\n",
    "\n",
    "##### Steps:\n",
    "1. Apply the json configuration found in [openai-backend-pool](../apimartifacts/backends/openai-backend-pool/backendInformation.json) via apiops\n",
    "2. Add a retry condition policy to the backend to configure the load balancing behavior on the API\n",
    "```xml\n",
    "    <backend>\n",
    "        <retry condition=\"@(context.Response.StatusCode == 429)\" count=\"2\" interval=\"1\" first-fast-retry=\"true\">\n",
    "            <forward-request />\n",
    "        </retry>\n",
    "    </backend>\n",
    "```\n",
    "3. Run the python cell below to observe the behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_runs = []\n",
    "runs = 20\n",
    "questions = [\n",
    "    \"What is climate change?\",\n",
    "    \"Explain climate change.\",\n",
    "    \"Describe the concept of climate change.\",\n",
    "    \"Tell me about global warming and its impact on the environment.\",\n",
    "    \"What causes climate change?\",\n",
    "    \"How does carbon dioxide contribute to global warming?\",\n",
    "    \"What are the effects of climate change on sea levels?\"\n",
    "]\n",
    "\n",
    "api_runs = client.run_queries(questions, runs, randomize=True) \n",
    "# Define the color map\n",
    "color_map = {\n",
    "    'East US': 'blue',\n",
    "    'North Central US': 'green',\n",
    "    'West US': 'red'\n",
    "    # Add more regions and their corresponding colors as needed\n",
    "}\n",
    "\n",
    "client.plot_load_balancing_results(api_runs, color_map, 'Load Balancing results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Analyzing Semantic Caching\n",
    "\n",
    "[![flow](../images/semantic-caching.gif)](semantic-caching.ipynb)\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "1. Connect an Azure Redis Cache instance to your API Management instance:\n",
    "    - Navigate to the Azure Portal.\n",
    "    - Go to Deployment & Infrastructure > External Cache.\n",
    "    - Follow the prompts to connect your Redis Cache instance.\n",
    "\n",
    "2. Create a backend for your embeddings model deployment:\n",
    "    - This involves setting up the backend service that will handle requests for your embeddings model.\n",
    "\n",
    "3. Add the following policy snippet to your inbound policy in Azure API Management:\n",
    "    ```xml\n",
    "    <azure-openai-semantic-cache-lookup score-threshold=\"0.8\" embeddings-backend-id=\"<embeddings-backend-name>\" embeddings-backend-auth=\"system-assigned\" />\n",
    "    ```\n",
    "    - `score-threshold`: A value between 0.0 and 1.0 that determines the similarity tolerance for cache matches. A higher score requires higher similarity.\n",
    "    - `embeddings-backend-id`: The identifier for your embeddings backend service.\n",
    "    - `embeddings-backend-auth`: Authentication method for the backend service, typically \"system-assigned\".\n",
    "\n",
    "4. Run through each test provided in the notebook and analyze the results to ensure the caching mechanism works as expected.\n",
    "\n",
    "For more detailed information, refer to the official documentation: [Azure API Management - How to Cache External Responses](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-cache-external)\n",
    "requiring higher similarity).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ Semantic Caching: Set 1 - Exact Matches (Control Group)\n",
    "\n",
    "This subsection of semantic caching is responsible for testing the first set of questions (Set 1: Exact Matches).\n",
    "\n",
    "The purpose of this test is to evaluate the performance and accuracy of the caching mechanism when dealing with exact match queries. This set serves as the control group, providing a baseline for comparison with other sets of questions that may involve more complex query patterns.\n",
    "\n",
    "__Expected Outcome__:\n",
    "- Latency should be extremely low for repeated queries.\n",
    "\n",
    "The results from this test will help in understanding the efficiency of the semantic caching system in handling straightforward, exact match queries and will serve as a reference point for further optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helper import run_queries, plot_results\n",
    "runs = 5\n",
    "api_runs = []  # Response Times for each run\n",
    "\n",
    "# Set 1: Exact Matches (Control Group)\n",
    "    # Expected Outcome:\n",
    "\t# ‚Ä¢\tLatency should be extremely low for repeated queries.\n",
    "questions = [\n",
    "    \"What is climate change?\",\n",
    "    \"What is climate change?\",\n",
    "    \"What is climate change?\",\n",
    "    \"What is climate change?\",\n",
    "    \"What is climate change?\"\n",
    "]\n",
    "\n",
    "api_runs = client.run_queries(questions, runs)\n",
    "response_times = [run[0] for run in api_runs]\n",
    "client.plot_results(response_times, 'Semantic Caching Performance (Set 1: Exact Matches)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ Semantic Caching: Set 2: Paraphrased/Slightly Related Matches\n",
    "\n",
    "This subsection of semantic caching is responsible for testing the first set of questions (Set 2: Paraphrased Matches)\n",
    "\n",
    "In this test, we evaluate the performance and accuracy of the caching mechanism when dealing with paraphrased queries. This set helps in understanding how well the caching system handles variations in query phrasing while still retrieving relevant cached responses.\n",
    "\n",
    "__Expected Outcome__:\n",
    "- Similar questions (1, 2, 3) should hit the cache.\n",
    "- Slightly reworded but related questions (4) may still hit the cache if the similarity threshold allows.\n",
    "- Questions with overlapping meanings (5 and 6) might hit the cache if configured for broad similarity.\n",
    "- Question 7 might generate a new response depending on threshold settings.\n",
    "\n",
    "The results from this test will provide insights into the robustness of the semantic caching system in handling paraphrased queries and its ability to maintain low latency and high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_runs = []  # Response Times for each run\n",
    "runs = 6\n",
    "questions = [\n",
    "    \"What is climate change?\",\n",
    "    \"Explain climate change.\",\n",
    "    \"Describe the concept of climate change.\",\n",
    "    \"Tell me about global warming and its impact on the environment.\",\n",
    "    \"What causes climate change?\",\n",
    "    \"How does carbon dioxide contribute to global warming?\",\n",
    "    \"What are the effects of climate change on sea levels?\"\n",
    "]\n",
    "\n",
    "api_runs = client.run_queries(questions, runs, randomize=False) \n",
    "response_times = [run[0] for run in api_runs]\n",
    "client.plot_results(response_times, 'Semantic Caching Performance (Set 2: Paraphrased Matches)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import run_queries, plot_load_balancing_results\n",
    "\n",
    "api_runs = []\n",
    "runs = 20\n",
    "questions = [\n",
    "    \"What is climate change?\",\n",
    "    \"Explain climate change.\",\n",
    "    \"Describe the concept of climate change.\",\n",
    "    \"Tell me about global warming and its impact on the environment.\",\n",
    "    \"What causes climate change?\",\n",
    "    \"How does carbon dioxide contribute to global warming?\",\n",
    "    \"What are the effects of climate change on sea levels?\"\n",
    "]\n",
    "\n",
    "api_runs = run_queries(questions, runs, randomize=True) \n",
    "# Define the color map\n",
    "color_map = {\n",
    "    'East US': 'blue',\n",
    "    'North Central US': 'green',\n",
    "    'West US': 'red'\n",
    "    # Add more regions and their corresponding colors as needed\n",
    "}\n",
    "\n",
    "plot_load_balancing_results(api_runs, color_map, 'Load Balancing results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Extract response times and regions from api_runs\n",
    "response_times = [run[0] for run in api_runs]\n",
    "regions = [run[1] for run in api_runs]\n",
    "\n",
    "client.plot_results(response_times, 'Semantic Caching Performance (Set 2: Paraphrased Matches)')\n",
    "# Calculate % distribution of load to the backend regions\n",
    "region_counts = pd.Series(regions).value_counts(normalize=True) * 100\n",
    "print(\"\\nüìä Percentage distribution of load to backend regions:\")\n",
    "print(region_counts)\n",
    "\n",
    "# Calculate average response times per region\n",
    "region_avg_response_times = pd.DataFrame(api_runs, columns=['Response Time', 'Region']).groupby('Region').mean()\n",
    "print(\"\\n‚è±Ô∏è Average response times per region:\")\n",
    "print(region_avg_response_times)\n",
    "\n",
    "# Identify potential causes for long response times\n",
    "threshold = np.percentile(response_times, 90)  # Define a threshold for long response times (90th percentile)\n",
    "long_response_times = [(time, region) for time, region in api_runs if time > threshold]\n",
    "\n",
    "print(\"\\nüö® Potential causes for long response times:\")\n",
    "for time, region in long_response_times:\n",
    "    print(f\"‚è∞ Response Time: {time:.2f} seconds, üåç Region: {region}\")\n",
    "    if time > 10:  # Arbitrary threshold for rate limiting/throttling\n",
    "        print(\"  - ‚ö†Ô∏è Potential cause: Rate limiting/throttling or policy misconfiguration\")\n",
    "\n",
    "        # Analyze time gaps between region calls to identify potential circuit breaker events\n",
    "        time_gaps = [j - i for i, j in zip(response_times[:-1], response_times[1:])]\n",
    "        circuit_breaker_events = [(gap, regions[idx + 1]) for idx, gap in enumerate(time_gaps) if gap > 5]\n",
    "        if circuit_breaker_events:\n",
    "            print(\"\\nüîç Circuit Breaker Analysis based on time gaps:\")\n",
    "            for gap, region in circuit_breaker_events:\n",
    "                print(f\"‚è∞ Time Gap: {gap:.2f} seconds, üåç Region: {region}\")\n",
    "                print(\"  - ‚ö†Ô∏è Potential cause: Circuit breaker triggered due to high response time gap\")\n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-gateway",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
